{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from iparse import IParser\n",
    "import evaluate\n",
    "import torch\n",
    "import torch_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "iparse = IParser(\"../models/gpn_tau_i2_back_kl_c0.01_7921957_dev=94.53.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_treebank = iparse.load_dev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "matrix = iparse.get_tag_dist(dev_treebank).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccg tags?\n",
    "tag_vocab = {'UNK': 0, '#': 1, '$': 2, \"''\": 3, ',': 4, '-LRB-': 5, '-RRB-': 6, '.': 7, ':': 8, 'CC': 9, 'CD': 10, 'DT': 11, 'EX': 12, 'FW': 13, 'IN': 14, 'JJ': 15, 'JJR': 16, 'JJS': 17, 'LS': 18, 'MD': 19, 'NN': 20, 'NNP': 21, 'NNPS': 22, 'NNS': 23, 'PDT': 24, 'POS': 25, 'PRP': 26, 'PRP$': 27, 'RB': 28, 'RBR': 29, 'RBS': 30, 'RP': 31, 'SYM': 32, 'TO': 33, 'UH': 34, 'VB': 35, 'VBD': 36, 'VBG': 37, 'VBN': 38, 'VBP': 39, 'VBZ': 40, 'WDT': 41, 'WP': 42, 'WP$': 43, 'WRB': 44, '``': 45}\n",
    "ind_to_tag = list(tag_vocab.keys())\n",
    "tagmap = nltk.tag.mapping.tagset_mapping('en-ptb', 'universal')\n",
    "uni = sorted(list(set(tagmap.values())))\n",
    "uni_to_ind = {tag: i for i, tag in enumerate(uni)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 46)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix = np.load('../models/tag_dist.npy').T\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there was only one UNK tag, ignoring for cleaner data\n",
    "\n",
    "# uni_mat = np.zeros((16, 11))\n",
    "# for j in range(matrix.shape[1]):\n",
    "#     ptb_tag = ind_to_tag[j]\n",
    "#     uni_tag = tagmap[ptb_tag]\n",
    "#     i = uni_to_ind[uni_tag]\n",
    "#     if i < 11:\n",
    "#         print(ptb_tag, ' converting to ', uni_tag)\n",
    "#         uni_mat[:, i] += matrix[:, j]\n",
    "# normalized = np.around((uni_mat.T / (np.sum(uni_mat, axis=1) + 1e-10)), 3).T\n",
    "\n",
    "# normalized by probability a tag is from a certain word class\n",
    "normalized = (matrix.T / (np.sum(matrix, axis=1) + 1e-10)).T\n",
    "df = pd.DataFrame(normalized, columns = ind_to_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(normalized, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNK</th>\n",
       "      <th>#</th>\n",
       "      <th>$</th>\n",
       "      <th>''</th>\n",
       "      <th>,</th>\n",
       "      <th>-LRB-</th>\n",
       "      <th>-RRB-</th>\n",
       "      <th>.</th>\n",
       "      <th>:</th>\n",
       "      <th>CC</th>\n",
       "      <th>...</th>\n",
       "      <th>VBD</th>\n",
       "      <th>VBG</th>\n",
       "      <th>VBN</th>\n",
       "      <th>VBP</th>\n",
       "      <th>VBZ</th>\n",
       "      <th>WDT</th>\n",
       "      <th>WP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>WRB</th>\n",
       "      <th>``</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008192</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.047316</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.118503</td>\n",
       "      <td>0.005791</td>\n",
       "      <td>0.028107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041243</td>\n",
       "      <td>0.010452</td>\n",
       "      <td>0.018362</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>0.013277</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.009746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008055</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.018412</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073648</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.003452</td>\n",
       "      <td>0.020713</td>\n",
       "      <td>0.041427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.031915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020686</td>\n",
       "      <td>0.005910</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>0.035461</td>\n",
       "      <td>0.023641</td>\n",
       "      <td>0.043144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.005910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>0.047966</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088646</td>\n",
       "      <td>0.011536</td>\n",
       "      <td>0.024894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>0.012143</td>\n",
       "      <td>0.027930</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>0.013965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.019465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104623</td>\n",
       "      <td>0.036496</td>\n",
       "      <td>0.034063</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.055961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010714</td>\n",
       "      <td>0.103571</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.014286</td>\n",
       "      <td>0.103571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0.006873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.044674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.013746</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007421</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>0.033395</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016698</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.063991</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.033623</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>0.018438</td>\n",
       "      <td>0.005423</td>\n",
       "      <td>0.009761</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.004338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.113306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>0.068607</td>\n",
       "      <td>0.009356</td>\n",
       "      <td>0.068607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025988</td>\n",
       "      <td>0.018711</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.004158</td>\n",
       "      <td>0.008316</td>\n",
       "      <td>0.012474</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>0.057173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.044259</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>0.002405</td>\n",
       "      <td>0.016838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041212</td>\n",
       "      <td>0.022290</td>\n",
       "      <td>0.036562</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.017319</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.005613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.012960</td>\n",
       "      <td>0.057299</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.030014</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.034106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021828</td>\n",
       "      <td>0.009550</td>\n",
       "      <td>0.010232</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.001364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.014337</td>\n",
       "      <td>0.004217</td>\n",
       "      <td>0.016656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029728</td>\n",
       "      <td>0.018343</td>\n",
       "      <td>0.025722</td>\n",
       "      <td>0.011807</td>\n",
       "      <td>0.013283</td>\n",
       "      <td>0.009699</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.002319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.091667</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.009797</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>0.054584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.013996</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.024493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027992</td>\n",
       "      <td>0.007698</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.010497</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>0.004899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.010981</td>\n",
       "      <td>0.007687</td>\n",
       "      <td>0.063141</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>0.023609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060029</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.009517</td>\n",
       "      <td>0.009883</td>\n",
       "      <td>0.020315</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.004392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    UNK         #         $        ''         ,     -LRB-     -RRB-         .  \\\n",
       "0   0.0  0.000000  0.008192  0.009605  0.047316  0.000847  0.001412  0.118503   \n",
       "1   0.0  0.000000  0.008055  0.001151  0.018412  0.001151  0.002301  0.002301   \n",
       "2   0.0  0.000000  0.004137  0.003546  0.031915  0.000000  0.000000  0.020686   \n",
       "3   0.0  0.000000  0.001821  0.009107  0.047966  0.001214  0.000000  0.088646   \n",
       "4   0.0  0.000000  0.009732  0.000000  0.024331  0.000000  0.002433  0.019465   \n",
       "5   0.0  0.000000  0.000000  0.010714  0.103571  0.003571  0.003571  0.071429   \n",
       "6   0.0  0.000000  0.020619  0.003436  0.051546  0.006873  0.000000  0.030928   \n",
       "7   0.0  0.000000  0.007421  0.011132  0.090909  0.000000  0.003711  0.033395   \n",
       "8   0.0  0.000000  0.014100  0.011931  0.063991  0.002169  0.002169  0.033623   \n",
       "9   0.0  0.000000  0.008316  0.004158  0.113306  0.000000  0.006237  0.068607   \n",
       "10  0.0  0.000000  0.005933  0.004009  0.044259  0.000641  0.000641  0.007377   \n",
       "11  0.0  0.000000  0.009550  0.012960  0.057299  0.002729  0.000682  0.030014   \n",
       "12  0.0  0.000422  0.009699  0.006747  0.052498  0.001476  0.000211  0.014337   \n",
       "13  0.0  0.000000  0.008333  0.008333  0.091667  0.016667  0.008333  0.008333   \n",
       "14  0.0  0.000700  0.009797  0.005598  0.054584  0.000000  0.000700  0.013996   \n",
       "15  0.0  0.000366  0.010981  0.007687  0.063141  0.000732  0.000366  0.022877   \n",
       "\n",
       "           :        CC  ...       VBD       VBG       VBN       VBP       VBZ  \\\n",
       "0   0.005791  0.028107  ...  0.041243  0.010452  0.018362  0.005932  0.013277   \n",
       "1   0.000000  0.009206  ...  0.073648  0.005754  0.003452  0.020713  0.041427   \n",
       "2   0.005910  0.017730  ...  0.106383  0.017730  0.035461  0.023641  0.043144   \n",
       "3   0.011536  0.024894  ...  0.059502  0.004857  0.007286  0.012143  0.027930   \n",
       "4   0.000000  0.000000  ...  0.104623  0.036496  0.034063  0.007299  0.055961   \n",
       "5   0.014286  0.103571  ...  0.025000  0.007143  0.003571  0.003571  0.003571   \n",
       "6   0.003436  0.044674  ...  0.010309  0.010309  0.000000  0.003436  0.013746   \n",
       "7   0.005566  0.020408  ...  0.016698  0.005566  0.001855  0.005566  0.012987   \n",
       "8   0.003254  0.028200  ...  0.024946  0.010846  0.018438  0.005423  0.009761   \n",
       "9   0.009356  0.068607  ...  0.025988  0.018711  0.012474  0.004158  0.008316   \n",
       "10  0.002405  0.016838  ...  0.041212  0.022290  0.036562  0.007858  0.017319   \n",
       "11  0.002729  0.034106  ...  0.021828  0.009550  0.010232  0.004093  0.008186   \n",
       "12  0.004217  0.016656  ...  0.029728  0.018343  0.025722  0.011807  0.013283   \n",
       "13  0.000000  0.041667  ...  0.033333  0.000000  0.050000  0.008333  0.008333   \n",
       "14  0.006298  0.024493  ...  0.027992  0.007698  0.006998  0.001400  0.010497   \n",
       "15  0.003477  0.023609  ...  0.060029  0.008419  0.009517  0.009883  0.020315   \n",
       "\n",
       "         WDT        WP       WP$       WRB        ``  \n",
       "0   0.003390  0.001271  0.000000  0.002401  0.009746  \n",
       "1   0.000000  0.000000  0.001151  0.000000  0.001151  \n",
       "2   0.000000  0.002364  0.000000  0.004728  0.005910  \n",
       "3   0.001214  0.003643  0.000000  0.003643  0.013965  \n",
       "4   0.000000  0.002433  0.000000  0.004866  0.000000  \n",
       "5   0.000000  0.003571  0.000000  0.000000  0.010714  \n",
       "6   0.000000  0.003436  0.000000  0.000000  0.006873  \n",
       "7   0.000000  0.000000  0.000000  0.000000  0.007421  \n",
       "8   0.002169  0.000000  0.001085  0.001085  0.004338  \n",
       "9   0.012474  0.001040  0.003119  0.003119  0.057173  \n",
       "10  0.005292  0.004009  0.000000  0.003368  0.005613  \n",
       "11  0.004093  0.003411  0.000000  0.002046  0.001364  \n",
       "12  0.009699  0.003373  0.000633  0.003373  0.002319  \n",
       "13  0.008333  0.000000  0.000000  0.000000  0.000000  \n",
       "14  0.001400  0.000700  0.000000  0.002099  0.004899  \n",
       "15  0.003660  0.001098  0.000000  0.000915  0.004392  \n",
       "\n",
       "[16 rows x 46 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize = (20,10))\n",
    "# ax.figure()\n",
    "ax.imshow(normalized, cmap='inferno', interpolation='nearest')\n",
    "ax.set_xticks(np.arange(46))\n",
    "ax.set_xticklabels(ind_to_tag)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3_wsj to\n",
      "[nltk_data]     /Users/thomas/nltk_data...\n",
      "[nltk_data]   Unzipping models/benepar_en3_wsj.zip.\n"
     ]
    }
   ],
   "source": [
    "import benepar\n",
    "benepar.download('benepar_en3_wsj')\n",
    "parser = benepar.Parser(\"benepar_en3_wsj\")\n",
    "original_parser = parser._parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_struct_dist(score):\n",
    "    length, length, tags = score.shape\n",
    "    score = score.reshape((1, length, length, tags))\n",
    "    score = score - score[..., :1]\n",
    "    dist = torch_struct.TreeCRF(torch.Tensor(score), lengths=torch.LongTensor(np.array([length])))\n",
    "    return dist.argmax\n",
    "    \n",
    "\n",
    "def get_skyline_trees(original_parser, iparse):\n",
    "\n",
    "    scores = original_parser.parse(\n",
    "        dev_treebank.without_gold_annotations(),\n",
    "        subbatch_max_tokens=2000,\n",
    "        return_scores=True\n",
    "    )\n",
    "    struc_scores = iparse.parser.parse(\n",
    "        dev_treebank.without_gold_annotations(),\n",
    "        subbatch_max_tokens=2000,\n",
    "        return_scores=True\n",
    "    )\n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        score = np.copy(scores[i])\n",
    "        struc_score = struc_scores[i]\n",
    "        struct_max = get_struct_dist(struc_score).max(-1).values[0]\n",
    "\n",
    "        score[struct_max < 1, 1:] = -1e10\n",
    "        amax = get_struct_dist(score)\n",
    "        amax[..., 0] += 1e-9\n",
    "        padded_charts = amax.argmax(-1)\n",
    "        padded_charts = padded_charts.detach().cpu().numpy()\n",
    "        chart = padded_charts[0]\n",
    "        leaves = dev_treebank[i].pos()\n",
    "        out = original_parser.decoder.tree_from_chart(chart, leaves=leaves)\n",
    "        yield out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fd7ee2c2612c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_skyline_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9efc22c8d3a9>\u001b[0m in \u001b[0;36mget_skyline_trees\u001b[0;34m(original_parser, iparse)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mreturn_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[0;32m---> 16\u001b[0;31m     struc_scores = iparse.parser.parse(\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdev_treebank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithout_gold_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0msubbatch_max_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/gum_parse_new/src/benepar2/parse_chart.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, examples, return_compressed, return_scores, subbatch_max_tokens, return_cats, tau, return_encoded)\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubbatch_max_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             res = subbatching.map(\n\u001b[0m\u001b[1;32m    805\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_encoded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/gum_parse_new/src/benepar2/subbatching.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(func, costs, max_cost, *data, **common_kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msubbatch_items\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0msubbatch_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubbatch_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcommon_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitem_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubbatch_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/gum_parse_new/src/benepar2/parse_chart.py\u001b[0m in \u001b[0;36m_parse_encoded\u001b[0;34m(self, examples, encoded, return_compressed, return_scores, return_cats, tau)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_encoded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mspan_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m                 \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid_token_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/gum_parse_new/src/benepar2/parse_chart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, tau, return_cat_logits, force_cats)\u001b[0m\n\u001b[1;32m    535\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mv_cp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 \u001b[0mannotations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_token_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m             \u001b[0;31m# Rearrange the annotations to ensure that the transition to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0;31m# fenceposts captures an even split between position and content.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/gum_parse_new/src/benepar2/partitioned_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, att_v, in_x)\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matt_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0matt_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_x\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/gum_parse_new/src/benepar2/partitioned_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, att_v, in_x)\u001b[0m\n\u001b[1;32m    195\u001b[0m         residual = self.linear2(self.ff_dropout(\n\u001b[1;32m    196\u001b[0m             self.activation(self.linear1(x))))\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_dropout_ff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_ff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trees = get_skyline_trees(original_parser, iparse)\n",
    "trees = list(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = original_parser.parse(\n",
    "    dev_treebank.without_gold_annotations(),\n",
    "    subbatch_max_tokens=2000,\n",
    "    return_scores=True\n",
    ")\n",
    "struc_scores = iparse.parser.parse(\n",
    "    dev_treebank.without_gold_annotations(),\n",
    "    subbatch_max_tokens=2000,\n",
    "    return_scores=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/anaconda3/lib/python3.8/site-packages/torch/distributions/distribution.py:44: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
      "  warnings.warn(f'{self.__class__} does not define `arg_constraints`. ' +\n"
     ]
    }
   ],
   "source": [
    "trees = []\n",
    "for i in range(len(scores)):\n",
    "    score = np.copy(scores[i])\n",
    "    struc_score = struc_scores[i]\n",
    "    struct_max = get_struct_dist(struc_score).max(-1).values[0]\n",
    "\n",
    "    score[struct_max < 1, 1:] = -1e10\n",
    "    length, length, tags = score.shape\n",
    "    charts = original_parser.decoder.charts_from_pytorch_scores_batched(\n",
    "        torch.Tensor(score.reshape(1, length, length, tags)), lengths=torch.LongTensor(np.array([length])))\n",
    "    leaves = dev_treebank[i].pos()\n",
    "    out = original_parser.decoder.tree_from_chart(charts[0], leaves=leaves)\n",
    "    trees.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_trees = original_parser.parse(\n",
    "    dev_treebank.without_gold_annotations(),\n",
    "    subbatch_max_tokens=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Recall=93.22, Precision=96.13, FScore=94.65, CompleteMatch=44.68)\n"
     ]
    }
   ],
   "source": [
    "evalb_dir=\"../EVALB_labeled/\"\n",
    "dev_fscore = evaluate.evalb(\n",
    "    evalb_dir, dev_treebank.trees, trees)\n",
    "print(dev_fscore)\n",
    "#dev=94.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Recall=95.68, Precision=95.76, FScore=95.72, CompleteMatch=56.05, TaggingAccuracy=97.16)\n"
     ]
    }
   ],
   "source": [
    "evalb_dir=\"../EVALB_labeled/\"\n",
    "dev_fscore = evaluate.evalb(\n",
    "    evalb_dir, dev_treebank.trees, original_trees)\n",
    "print(dev_fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
